---
title: "\"oal\" - Occasional Active Learning"
subtitle: "An open source implementation of e-discovery active learning"
author: "Andre Abadi"
date: "`r Sys.Date()`"
output:
  github_document:
    df_print: kable
  pdf_document:
    fig_caption: true # render captions for figures
    extra_dependencies: ["float"] # floating images
    df_print: kable # print dataframes as kables automatically
---

```{r setup_output, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, # knit code (default = TRUE)
  message = FALSE, # knit messages (default = TRUE)
  warning = FALSE, # knit warnings (default = TRUE)
  cache = TRUE, # cache knitted chunks (default = FALSE)
  fig.align = "centre", # align figures to centre
  fig.pos = "H", # hard-here-mode for plots
  out.extra = ""
)
```

```{r setup_code}
set.seed(1)
library(tidyverse)
library(viridis)
library(tidymodels)
tidymodels_prefer()
library(textrecipes)
library(discrim)
library(naivebayes)
library(keras)
```

## Introduction

The aim of this project is to develop a predictive model for binary classification of legal documents. Many e-discovery platforms implement such predictive models but are proprietary. This project aims to provide an open source equivalent, not for production (necessarily) but for educational purposes. 


### References

Our primary text reference for this project is *Supervised Machine Learning for Text Analysis in R* (**SMLTAR**) by Emil Whitfield and Julia Silge. This text is available as an interactive resource at [smltar.com](https://smltar.com/). Where not otherwise cited, quotes are from this primary text reference.

### Style and Conventions

- Code is written using `snake_case` and attempts to follow the guidance of the online resource [*R for Data Science (2e)*](https://r4ds.hadley.nz/) by Wickham, Ã‡etinkaya-Rundel, and Grolemund.
- We also adopt the [*Tidyverse Style Guide*](https://style.tidyverse.org/) with a preference toward piping operations where possible for code clarity.
- We use the native pipe where possible and fall back to the [*magrittr*](https://magrittr.tidyverse.org/) pipe where necessary.
- We also use the [*viridis*](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html), colourising charts to improve accessibility of visualisations for readers with colour vision deficiency.
- We use [Tidymodels](https://www.tidymodels.org/) for our analysis, as it promotes good modelling practices by streamlining workflows, minimizing data leakage risks, and ensuring reproducibility.

## Target

We hope to meet or exceed the following metrics, which were calculated on unseen test data. It should be noted that most of our modelling sets aside the unseen test data, so most confusion matrices are not directly comparable to the below in terms of utilised data, however all still provide a good subject assessment of model performance.

```{r ref_metrics}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
ref <-
  read_csv("data/reference.csv",
           show_col_types = FALSE) |>
  select(-full,-train,-id) |>
  filter(!is.na(test)) |>
  mutate(
    truth = factor(case_when(
      test == "irrelevant" ~ "0",
      test == "relevant" ~ "1"
    ), levels = c("0", "1")),
    .pred_1 = (score + 1) / 2,
    .pred_0 = 1 - .pred_1,
    .pred_class = factor(if_else(.pred_1 >= 0.5, "1", "0"), 
                        levels = c("0", "1"))
  ) |>
  select(truth, starts_with(".pred"))
ref |> 
  metric_set(accuracy, roc_auc)(
    truth = truth,
    estimate = .pred_class,
    .pred_0
  ) |>
  as.data.frame()
ref |> 
  conf_mat(truth, .pred_class) |>
  autoplot(type = "heatmap") +
  scale_fill_viridis_c(begin = 0.3) +
  scale_x_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  scale_y_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  labs(
    title = "Confusion Matrix for Reference Model",
    x = "Truth",
    y = "Prediction"
  ) |> suppressWarnings()
```

Knowing that the above reference model was fitted over a perfectly balanced training set (50:50 relevant/irrelevant) it is notable that the model produces a very unbalanced result. The model appears to be effective at classifying irrelevant documents, but not very effective at classifying relevant documents. 

## Import

```{r import}
require(tidyverse)
enron_raw <-
  read_csv("data/enron_5k_inboard.csv",
           show_col_types = FALSE) |>
  select(-md5) |>
  mutate(relevant = as_factor(relevant),
         train_test = as_factor(train_test))
enron_raw |> glimpse()
```

## Prepare

### Tidy

```{r tidy}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
enron_tidy <-
  enron_raw |>
  select(-subject,-date,-from,-to) |>
  filter(!is.na(train_test))
enron_tidy |> glimpse()
```

### Exploratory Data Analysis

[SMLTAR 8.1](https://smltar.com/dldnn#kickstarter)

```{r histo}
require(tidyverse)
require(viridis)
enron_tidy |>
  mutate(content_length = str_count(content, "\\w+")) |>
  filter(!is.na(content_length)) |>
  ggplot(aes(x = content_length)) +
  geom_histogram(
    aes(fill = after_stat(count)), 
    binwidth = 0.1,
  ) +
  scale_fill_viridis_c() +
  labs(
    title = "Distribution of content length.",
    x = "Content Length (Words)",
    y = "Log10( Count )",
    fill = "Frequency"
  ) +
  theme(legend.position = "none") +
  scale_x_log10()
```

### Splits and Folds

```{r splits_and_folds}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
train_indices <- 
  which(enron_tidy$train_test == "train")
test_indices <- 
  which(enron_tidy$train_test == "test")
enron_split <- 
  make_splits(list(analysis = train_indices, 
                   assessment = test_indices), 
              enron_tidy)
rm(train_indices,test_indices)
enron_train <-
  training(enron_split)
enron_test <-
  testing(enron_split)
enron_folds <-
  vfold_cv(enron_train)
```

## Modelling

### Model 00 - Null Model

[SMLTAR 7.2](https://smltar.com/mlclassification#classnull)

For our null model, we create our first recipe. It is a basic recipe for ingestion to the null model, and is unfiltered tokenization truncated at the top 1000 with TF-IDF subsequently applied. We do not propose to use it for any true modelling.

```{r r00}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
recipe_null <-
  recipe(relevant ~ content, data = enron_train) |>
  step_tokenize(content) |>
  step_tokenfilter(content, max_tokens = 1e3) |>
  step_tfidf(content)
recipe_null |>
  prep() |>
  bake(new_data = NULL) |>
  select(contains("content")) |>
  slice_head(n = 5) |>
  select_if(~any(. != 0)) |>
  select(1:5) |>
  as.data.frame()
```

>"a 'null model' or baseline model, [is] a simple, non-informative model that always predicts the largest class for classification. Such a model is perhaps the simplest heuristic or rule-based alternative that we can consider as we assess our modelling efforts."

```{r m00}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
null_model <-
  null_model() |>
  set_engine("parsnip") |>
  set_mode("classification")
null_workflow <-
  workflow() |>
  add_recipe(recipe_null) |>
  add_model(null_model)
null_fit <-
  null_workflow |>
  fit_resamples(
    resamples = enron_folds,
    control = control_resamples(save_pred = TRUE)
  )
null_metrics <-
  null_fit |> 
  collect_metrics()
null_metrics |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
rm(
  null_model,
  null_workflow,
  null_fit,
  null_metrics
)
```

The null model above shows a ROC_AUC indicating that without any predictive modelling, the null model guesses the correct classification that proportion of the time. This is consistent with the 50:50 breakdown of labelled training data. It is akin to flipping a coin, and provides our baseline that we hope to improve upon.

### Model 01 - Naive Bayes

[SMLTAR 7.1.1](https://smltar.com/mlclassification#classfirstmodel)

For this model, we implement a new tune-able recipe. This second recipe is similar to the first, with unfiltered tokenization, but this time with the top *n* tokens set for tuning between 500 and 5000, and TF-IDF applied subsequently as with the previous recipe.

```{r r01}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
recipe_tfidf <-
  recipe(relevant ~ content, data = enron_train) |>
  step_tokenize(content) |>
  step_tokenfilter(content, max_tokens = tune()) |>
  step_tfidf(content)
```

A Naive Bayes classifier applies Bayes' Theorem to predict class membership by calculating conditional probabilities, making the "naive" assumption that features are conditionally independent of each other. Despite this simplifying assumption, the classifier is remarkably effective, particularly for text classification tasks. It works by learning the probability of each feature occurring within each class during training, then combining these probabilities with prior class probabilities to predict the most likely class for new instances.

```{r m01}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
require(discrim)
require(naivebayes)
bayes_model <-
  naive_Bayes() |>
  set_mode("classification") |>
  set_engine("naivebayes")
bayes_workflow <-
  workflow() |>
  add_recipe(recipe_tfidf) |>
  add_model(bayes_model)
bayes_grid <-
  grid_regular(
    max_tokens(range = c(100,5000)),
    levels = 5
  )
bayes_resamples <-
  bayes_workflow |>
  tune_grid(
    resamples = enron_folds,
    grid = bayes_grid,
    control = control_resamples(save_pred = TRUE)
  )
bayes_best <-
  bayes_resamples |>
  select_by_one_std_err(metric = "roc_auc", -max_tokens)
bayes_final_workflow <-
  finalize_workflow(
    bayes_workflow,
    bayes_best 
  )
bayes_final_fit <-
  bayes_final_workflow |>
  fit_resamples(
    resamples = enron_folds,
    control = control_resamples(save_pred = TRUE)
  )
bayes_metrics <-
  bayes_final_fit |>
  collect_metrics()
bayes_roc <- 
  bayes_final_fit |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(relevant, .pred_1) |>
  mutate(model = "Naive Bayes")
bayes_metrics |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
bayes_final_fit |>
  collect_predictions() |> 
  conf_mat(truth = relevant, 
          estimate = .pred_class) |>
  autoplot(type = "heatmap") +
  scale_fill_viridis_c(begin = 0.3) +
  scale_x_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  scale_y_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  labs(
    title = "Confusion Matrix for Model 00 - Naive Bayes",
    x = "Truth",
    y = "Prediction"
  )
rm(
  bayes_model,
  bayes_workflow,
  bayes_grid,
  bayes_resamples,
  bayes_best,
  bayes_final_workflow,
  bayes_metrics
)
```

Analysing the above results we see that there is reasonable but unbalanced performance. The model was effective at classifying the irrelevant documents but did not perform well at classifying the relevant documents. Given the balanced training data, this suggests the model did not perform well at this task.

### Model 02 - LASSO

[SMLTAR 7.3](https://smltar.com/mlclassification#comparetolasso)

For this model we re-use the same recipe as we used for Model 01 - Naive Bayes. This recipe had a tune-able token count and we use it for this model also.

>"Regularized linear models are a class of statistical model that can be used in regression and classification tasks. Linear models are not considered cutting edge in NLP research, but are a workhorse in real-world practice. Here we will use a lasso regularized model (Tibshirani 1996), where the regularization method also performs variable selection. In text analysis, we typically have many tokens, which are the features in our machine learning problem."

```{r m02}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
lasso_model <-
  logistic_reg(penalty = tune(), mixture = 1) |>
  set_mode("classification") |>
  set_engine("glmnet")
lasso_workflow <- 
  workflow() |>
  add_recipe(recipe_tfidf) |>
  add_model(lasso_model)
lasso_grid <- 
  grid_regular(
    max_tokens(range = c(500, 2000)),
    penalty(range = c(-4, 0)),
    levels = c(3, 3)
  )
lasso_resamples <-
  lasso_workflow |>
  tune_grid(
    resamples = enron_folds,
    grid = lasso_grid,
    control = control_resamples(save_pred = TRUE)
  )
lasso_best <-
  lasso_resamples |>
  select_by_one_std_err(metric = "roc_auc", -penalty)
lasso_final_workflow <-
  finalize_workflow(
    lasso_workflow,
    lasso_best
  )
lasso_final_fit <-
  lasso_final_workflow |>
  fit_resamples(
    resamples = enron_folds,
    control = control_resamples(save_pred = TRUE)
  )
lasso_final_fit |>
  collect_metrics() |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
lasso_final_fit |>
  collect_predictions() |> 
  conf_mat(truth = relevant, 
          estimate = .pred_class) |>
  autoplot(type = "heatmap") +
  scale_fill_viridis_c(begin = 0.3) +
  scale_x_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  scale_y_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  labs(
    title = "Confusion Matrix for Model 01 - LASSO Regularised Model",
    x = "Truth",
    y = "Prediction"
  )
rm(
  lasso_model,
  lasso_workflow,
  lasso_grid,
  lasso_resamples,
  lasso_best,
  lasso_final_workflow
)
```

In the above results we see that the LASSO model produces more balanced classifications between Relevant and Irrelevant. The regularisation penalty was tuned to achieve the above result. We see a small increase in the accuracy metric, and a significant improvement in the ROC_AUC metric. This, combined with the subjective assessment via the confusion matrix suggests an improved model overall.

### Model 03 - Dense Neural Network (DNN)

[SMLTAR 8.1](https://smltar.com/dldnn#firstdlclassification)

We implement "a densely connected neural network. This is typically not a model that will achieve the highest performance on text data, but it is a good place to start to understand the process of building and evaluating deep learning models for text. We can also use this type of network architecture as a bridge between the bag-of-words approaches. ... Deep learning allows us to incorporate not just word counts but also word sequences and positions."

```{r recipe_onehot}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
max_words <- 2e4
max_length <- 5000
min_wordcount <- 2
onehot_recipe <- 
  recipe(~ content, data = enron_train) |>
  step_text_normalization(content) |> 
  step_tokenize(content) |>
  #step_stopwords(content) |>
  step_tokenfilter(content, 
                  max_tokens = max_words,
                  min_times = min_wordcount) |>
  step_sequence_onehot(content, 
                      sequence_length = max_length,
                      prefix = "")
onehot_recipe |>
  prep() |>
  tidy(4) |>
  slice_sample(n = 10) |>
  as.data.frame() |>
  select(-id,-terms)
onehot_prep <-
  onehot_recipe |>
  prep()
onehot_x <-
  onehot_prep |>
  bake(new_data = NULL, 
       composition = "matrix")
onehot_y <-
  enron_train$relevant |>
  as.numeric() |>
  matrix(ncol = 1)
```

[SMLTAR 8.2.3](https://smltar.com/dldnn#simple-flattened-dense-network)

With our new one-hot encoding recipe ready, we create the model object.

- Input layers:
  - Embedding layer of 128 dimensions.
- Hidden layers:
  - Flatten layers (turns 2D embedding layer shape to 1D).
  - Dropout of 50%.
  - Dense layer of 128 units and ReLU activation.
- Output layers:
  - Dense layer of 1 unit and sigmoid activation.


```{r dnn}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(keras)
logging_level <- tensorflow::tf$compat$v1$logging$ERROR
tensorflow::tf$compat$v1$logging$set_verbosity(logging_level)
rm(logging_level)
Sys.setenv(TF_CPP_MIN_LOG_LEVEL = "2")
dnn <- 
  keras_model_sequential() %>%
  layer_embedding(input_dim = max_words + 1,
                  output_dim = 12,
                  input_length = max_length) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
dnn |>
  compile(
    optimizer = "adam",
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
dnn_fit <-
  dnn |>
  fit(
    x = onehot_x,
    y = onehot_y,
    batch_size = 512,
    epochs = 20,
    validation_split = 0.25,
    verbose = FALSE
  )
plot(dnn_fit) +
  scale_color_viridis_d(end = 0.8) +
  scale_fill_viridis_d(end = 0.8) +
  labs(
    x = "Epoch",
    y = NULL,
    color = "Dataset",
    linetype = "Dataset",
    title = "DNN Training History",
    subtitle = "Loss and Accuracy over Training Epochs"
  ) +
  theme(legend.position = "right")
```


## Comparison

We show the ROC curve of our models below.

```{r roc_compare}
#| fig.width: 7
#| fig.asp: 0.618
#| out.width: "100%"
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
ref_roc <-
  ref |>
  mutate(id = "Fold01") |>
  group_by(id) |>
  roc_curve(truth = truth, .pred_0) |>
  mutate(model = "Reference Model")
bayes_roc <- 
  bayes_final_fit |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(truth = relevant, .pred_0) |>
  mutate(model = "Model 01 - Naive Bayes")
lasso_roc <- 
  lasso_final_fit |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(truth = relevant, .pred_0) |>
  mutate(model = "Model 02 - LASSO")
bind_rows(ref_roc, bayes_roc, lasso_roc) |>
  group_by(model, 
          specificity = round(specificity, 2)) |>
  summarize(
    sensitivity_mean = mean(sensitivity),
    sensitivity_lower = quantile(sensitivity, 0.025),
    sensitivity_upper = quantile(sensitivity, 0.975),
    .groups = "drop"
  ) |>
  ggplot(aes(x = 1 - specificity, 
             y = sensitivity_mean, 
             color = model,
             fill = model)) +
  geom_line(linewidth = 1) +
  geom_abline(lty = 3) +
  coord_equal() +
  scale_color_viridis_d(end = 0.8) +
  scale_fill_viridis_d(end = 0.8) +
  labs(
    title = "ROC Curve Comparison",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Model",
    fill = "Model"
  ) +
  theme(
    legend.position = c(0.4, 0),
    legend.justification = c(0, 0),
    legend.direction = "vertical",
    legend.box = "vertical",
    legend.background = element_rect(fill = "white", color = NA),
    aspect.ratio = 0.7
  )
rm(
  ref_roc,
  bayes_roc,
  lasso_roc
)
```
