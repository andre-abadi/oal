---
title: "\"oal\" - Occasional Active Learning"
subtitle: "An open source implementation of e-discovery active learning"
author: "Andre Abadi"
date: "`r Sys.Date()`"
output:
  github_document:
    df_print: kable
  pdf_document:
    fig_caption: true # render captions for figures
    extra_dependencies: ["float"] # floating images
    df_print: kable # print dataframes as kables automatically
---

```{r setup_output, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, # knit code (default = TRUE)
  message = FALSE, # knit messages (default = TRUE)
  warning = FALSE, # knit warnings (default = TRUE)
  cache = TRUE, # cache knitted chunks (default = FALSE)
  fig.align = "centre", # align figures to centre
  fig.pos = "H", # hard-here-mode for plots
  out.extra = ""
)
```

```{r setup_code}
set.seed(1)
library(tidyverse)
library(viridis)
library(tidymodels)
tidymodels_prefer()
library(textrecipes)
library(discrim)
library(naivebayes)
```

## Introduction

The aim of this project is to develop a predictive model for binary classification of legal documents. Many e-discovery platforms implement such predictive models but are proprietary. This project aims to provide an open source equivalent, not for production (necessarily) but for educational purposes. We hope to meet or exceed the following metrics:

| Metric | Estimate |
| --- | --- |
| Accuracy (50% threshold) | 0.74 |
| ROC-AUC | 0.77 |

### References

Our primary text reference for this project is *Supervised Machine Learning for Text Analysis in R* (**SMLTAR**) by Emil Whitfield and Julia Silge. This text is available as an interactive resource at [smltar.com](https://smltar.com/).

### Style and Conventions

- Code is written using `snake_case` and attempts to follow the guidance of the online resource [*R for Data Science (2e)*](https://r4ds.hadley.nz/) by Wickham, Ã‡etinkaya-Rundel, and Grolemund.
- We also adopt the [*Tidyverse Style Guide*](https://style.tidyverse.org/) with a preference toward piping operations where possible for code clarity.
- We use the native pipe where possible and fall back to the [*magrittr*](https://magrittr.tidyverse.org/) pipe where necessary.
- We also use the [*viridis*](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html), colourising charts to improve accessibility of visualisations for readers with colour vision deficiency.
- We use [Tidymodels](https://www.tidymodels.org/) for our analysis, as it promotes good modelling practices by streamlining workflows, minimizing data leakage risks, and ensuring reproducibility.

## Import

```{r import}
require(tidyverse)
enron_raw <-
  read_csv("data/enron_5k_inboard.csv",
           show_col_types = FALSE) |>
  select(-md5) |>
  mutate(relevant = as_factor(relevant),
         train_test = as_factor(train_test))
enron_raw |> glimpse()
```
## Prepare

### Tidy

```{r tidy}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
enron_tidy <-
  enron_raw |>
  select(-subject,-date,-from,-to)
enron_tidy |> glimpse()
```

### Splits and Folds

```{r splits_and_folds}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
train_indices <- 
  which(enron_tidy$train_test == "train")
test_indices <- 
  which(enron_tidy$train_test == "test")
enron_split <- 
  make_splits(list(analysis = train_indices, 
                   assessment = test_indices), 
              enron_tidy)
rm(train_indices,test_indices)
enron_train <-
  training(enron_split)
enron_test <-
  testing(enron_split)
enron_folds <-
  vfold_cv(enron_train)
```

### Preprocessing Recipe

```{r recipe_01}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
recipe_01 <-
  recipe(relevant ~ content, data = enron_train) |>
  step_tokenize(content) |>
  step_tokenfilter(content, max_tokens = 1e3) |>
  step_tfidf(content)
recipe_01
```

## Modelling

### Model 00 - Null Model

>"a 'null model' or baseline model, [is] a simple, non-informative model that always predicts the largest class for classification. Such a model is perhaps the simplest heuristic or rule-based alternative that we can consider as we assess our modelling efforts."

```{r m00_null}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
m00_null_model <-
  null_model() |>
  set_engine("parsnip") |>
  set_mode("classification")
m00_null_wf <-
  workflow() |>
  add_recipe(recipe_01) |>
  add_model(m00_null_model)
m00_null_results <-
  m00_null_wf |>
  fit_resamples(enron_folds)
m00_null_results |> 
  collect_metrics() |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
```

### Model 01 - Naive Bayes

A Naive Bayes classifier applies Bayes' Theorem to predict class membership by calculating conditional probabilities, making the "naive" assumption that features are conditionally independent of each other. Despite this simplifying assumption, the classifier is remarkably effective, particularly for text classification tasks. It works by learning the probability of each feature occurring within each class during training, then combining these probabilities with prior class probabilities to predict the most likely class for new instances.

```{r m01_bayes}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
require(discrim)
require(naivebayes)
m01_bayes_model <-
  naive_Bayes() |>
  set_mode("classification") |>
  set_engine("naivebayes")
m01_bayes_wf <-
  workflow() |>
  add_recipe(recipe_01) |>
  add_model(m01_bayes_model)
m01_bayes_results <-
  m01_bayes_wf |>
  fit_resamples(enron_folds)
m01_bayes_results |> 
  collect_metrics() |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
```
