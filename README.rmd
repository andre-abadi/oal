---
title: "\"oal\" - Occasional Active Learning"
subtitle: "An open source implementation of e-discovery active learning"
author: "Andre Abadi"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, # knit code (default = TRUE)
  message = FALSE, # knit messages (default = TRUE)
  warning = FALSE, # knit warnings (default = TRUE)
  cache = FALSE, # cache knitted chunks (default = FALSE)
  fig.align = "centre", # align figures to centre
  #fig.pos = "H", # hard-here-mode for plots
  out.extra = ""
)
```

```{r required_libraries}
library(tidyverse) # read_csv, mutate
library(tidytext) # unnest_tokens
library(textclean) # replace_contraction
require(stringi) # stri_trans_ncf
require(stringr) # str_squish
library(torch)
library(data.table) # fread
```

## Introduction

The aim of the **oal** project (**This Project**) is to implement active learning for eDiscovery using the R programming language, exclusively for learning purposes. This initiative focuses on developing a system to efficiently process the Enron dataset, a collection comprising extensive email communications relevant to the infamous collapse of Enron.

## Import

We load a specific subset of the Enron e-discovery dataset, focusing only on the doc_id, relevant, and content columns. The dataset is then converted to a tibble format and filtered to remove any rows where the relevant value is NA. Finally, an excerpt of the content from a specific document is displayed, wrapped to a width of 75 characters for readability.
```{r import}
require(data.table) # fread
require(tidyverse) # as_tibble
enron <-
  fread(
    "data/enron_edisco.csv", # file to read
    select = c("doc_id", "relevant", "content") # only these columns
  ) |> 
  as_tibble() |> # tidyverse
  filter(!is.na(relevant)) # drop rows where relevant = NA
# excerpt
enron$content[23] |>
  str_wrap(width = 75) |>
  cat()
```

## Clean

We perform extensive preprocessing on the content column of the Enron dataset. We apply various Regex patterns to remove non-word characters, URLs, email addresses, and file paths. The content is then converted to lower case, contractions are expanded, and Unicode normalization is applied. We remove stop words and clean up whitespace to ensure no extra spaces remain. Finally, we filter out rows with empty content after preprocessing. We display the cleaned content of a specific document as an excerpt.

```{r content_words_only}
require(tidyverse) # mutate
require(tidytext) # stop_words
require(textclean) # replace_contraction
require(stringi) # stri_trans_ncf
require(stringr) # str_squish
stop_words_pattern <- 
  paste0("\\b(", paste(stop_words$word, collapse = "|"), ")\\b")
regex_patterns <- 
  list(
    "[^a-zA-Z]",  # non-words
    "https?://\\S+|www\\.\\S+",  # URLs
    "[\\w._%+-]+@[\\w.-]+\\.[a-zA-Z]{2,}",  # emails
    "[\\w\\-]+(/|\\\\)[\\w\\-]+(/|\\\\)?"  # file paths
)
combined_regex <- 
  paste0("(", paste0(regex_patterns, collapse = "|"), ")")
enron <-
  enron |>
  mutate(
    content = str_replace_all(content, combined_regex, " "), # apply regex
    content = tolower(content), # to lower case
    content = replace_contraction(content),
    content = stri_trans_nfc(content), # unicode normalisation
    content = str_replace_all(content, stop_words_pattern, ""),
    content = str_squish(content) # max 1 whitespace, remove start and end
  ) |>
  filter(content != "" & !is.na(content)) # remove now-empty content rows
rm(
  regex_patterns,
  combined_regex,
  stop_words_pattern
)
# excerpt
enron$content[23] |>
  str_wrap(width = 75) |>
  cat()
```
## Message Length Survey

We visualize the distribution of message lengths in the Enron dataset. Each message's length is calculated, and the distribution is plotted on a logarithmic scale to better handle the wide range of message lengths. The histogram is color-coded using the viridis color palette, enhancing the visualization by mapping the frequency of message lengths to a gradient. The plot provides insights into how the lengths of messages are distributed across the dataset.

```{r length_distro}
require(ggplot2)
enron %>%
  mutate(content_length = nchar(content)) %>%
  ggplot(aes(x = content_length, fill = ..count..)) +
  geom_histogram(binwidth = 0.5,) +
  #scale_x_log10() +
  scale_fill_viridis_c(option = "viridis", end = 0.8) +
  labs(title = "Distribution of Message Lengths", 
       x = "Message Length (characters)", 
       y = "Frequency") +
  scale_x_continuous(trans='log')
```

## Truncate

We truncate the messages as part of development to improve our ability iterate training-evaluation cycles and understand hyperparameter turning. We envisage removing this pre-processing step at some stage once accuracy is at competitive levels.

```{r truncate}
require(tidyverse) # mutate
truncate_content <- function(content, max_length = 1000) {
  if (nchar(content) > max_length) {
    truncated <- substr(content, 1, max_length)
    last_space <- max(gregexpr(" ", truncated)[[1]])
    if (last_space > 0) {
      truncated <- substr(truncated, 1, last_space - 1)
    }
  } else {
    truncated <- content
  }
  return(truncated)
}
enron <- enron |> 
  mutate(content = sapply(content, truncate_content))
rm(truncate_content)
# excerpt
enron$content[23] |>
  str_wrap(width = 75) |>
  cat()
```

## Training/Testing Split

We split the classified data into training and testing subsets at an 80:20 ratio. This means that the model can train on 80% of the available data while 20% is held out and not seen by the model during training, so that it can be used to evaluate model performance fairly. We show ratio of relevant and not-relevant documents in each set as a means of checking that undertaking this process yielded sets that were balanced across the binary classification.

```{r train-test}
require(tidyverse)
set.seed(1)
train_ratio <- 0.8
enron <- enron %>%
  mutate(set = ifelse(runif(n()) < train_ratio, "train", "test"))
enron_train <- 
  enron %>% 
  filter(set == "train") %>% 
  select(-set)
enron_test  <- 
  enron %>% 
  filter(set == "test") %>% 
  select(-set)
enron_train %>%
  count(relevant) %>%
  mutate(percentage = n / sum(n) * 100)
enron_test %>%
  count(relevant) %>%
  mutate(percentage = n / sum(n) * 100)
rm(train_ratio
)
```

## Tokenize

We tokenize the training set by creating a vocabulary of all words used therein, in frequency order i.e. lower vocabulary indexes indicate higher frequency of the word in the data. We provide the same sample message again in tokenized form, to show the effect of this step, which makes the messages readable by the model.

```{r tokenize}
require(tidyverse)
enron_train <- 
  enron_train |> 
  mutate(tokens = str_split(content, "\\s+")) # create vector of all words
vocab <- 
  enron_train$tokens |> 
  unlist() |> 
  table() |> 
  sort(decreasing = TRUE) |> 
  names()
word_to_index <- 
  setNames(seq_along(vocab), vocab)
enron_train <- enron_train |> 
  mutate(
    indices = map(tokens, ~ word_to_index[.x]),
    tokens = NULL)
na_indices <- 
  sapply(enron_train$indices, 
         function(x) any(is.na(x)))
enron_train <-
  enron_train[!na_indices, ]
enron_test <- 
  enron_test |> 
  mutate(tokens = str_split(content, "\\s+")) |> # tokenize
  mutate(indices = map(tokens, ~ word_to_index[.x]), tokens = NULL)
na_indices <- sapply(enron_test$indices, function(x) any(is.na(x)))
enron_test <- enron_test[!na_indices, ]
rm(na_indices)
rm(word_to_index)
# exerpt
enron_train |> filter(doc_id == "ENR.0001.0017.0141") |> pull(indices)
```

## Convert to Tensors

Mainly procedural, we must convert these plain lists into a format known as Tensors, which is a data storage format designed specifically for machine learning and the torch framework. The same message is shown again, with only the vocabulary word indexes in the order they appear in the message. Duplicates are allowed, and this effectively completes the conversion of the message from human readable format to machine-learning friendly format.

```{r tensors}
require(torch)  # for tensor operations
set.seed(1)
torch_manual_seed(1)
#
# convert to tensors
#
pad <- length(vocab) + 1
indices_tensors_train <- 
  lapply(enron_train$indices, 
         torch_tensor, 
         dtype = torch_int64())
tensor_data_train <- 
  nn_utils_rnn_pad_sequence(indices_tensors_train, 
                            batch_first = TRUE,
                            padding_value = pad)
tensor_labels_train <- 
  torch_tensor(ifelse(enron$relevant == -1, 0, enron$relevant), 
               dtype = torch_int64())
indices_tensors_test <- 
  lapply(enron_test$indices, 
         torch_tensor, 
         dtype = torch_int64())
tensor_data_test <- 
  nn_utils_rnn_pad_sequence(indices_tensors_test, 
                            batch_first = TRUE,
                            padding_value = pad)
tensor_labels_test <- 
  torch_tensor(ifelse(enron_test$relevant == -1, 0, enron_test$relevant), 
               dtype = torch_int64())
rm(
  indices_tensors_train,
  indices_tensors_test,
  pad
)
#
# custom class and use it
#
custom_class <- dataset(
  initialize = function(tensors, labels) {
    self$tensors <- tensors  # Store the input tensors
    self$labels <- labels  # Store the labels
  },
  .getitem = function(index) {
    input_tensor <- self$tensors[index, ]
    label <- self$labels[index]
    return(list(input_tensor, label)) 
  },
  .length = function() {
    return(self$tensors$size(1))
  }
)
enron_tensor_dataset_train <- 
  custom_class(tensor_data_train, tensor_labels_train)
enron_tensor_dataset_test <- 
  custom_class(tensor_data_test, tensor_labels_test)
# exerpt
row_index <-
  which(enron_train$doc_id == "ENR.0001.0017.0141")
enron_tensor_dataset_train$.getitem(row_index)[[1]][enron_tensor_dataset_train$.getitem(row_index)[[1]] != length(vocab) + 1]
rm(row_index)
#enron_tensor_dataset_train$.getitem(row_index)[[2]]
```

## Train and Evaluate Model

With the data preprocessed and converted into the necessary formats, we can train a neural network on the training data and then evaluate it on the unseen test data. We have several parameters and hyperparameters available when undertaking this process, and we list them below with a short description.

- `embedding_dim`: Determines the size of the word embeddings, where each word in the vocabulary is mapped to a vector of this length.
  - Higher dimensions capture more nuances but increase computational cost.
- `n_hidden`: Sets the number of hidden units in the neural network’s hidden layer(s).
  - This controls the model’s capacity to learn complex patterns.
- `batch_size`: Specifies the number of samples processed before the model’s internal parameters are updated.
  - Larger batch sizes provide a more stable gradient estimate but require more memory.
- `learn_rate`: Controls how much the model's parameters are adjusted with respect to the loss gradient.
  - A smaller learning rate can lead to more precise convergence but slower training.
- `num_epochs`: Defines how many times the entire training dataset passes through the model.
  - More epochs allow the model to learn better but increase the risk of overfitting if too many epochs are used.

With the parameters and hyperparameters loaded, the model trains accordingly. The losses are rendered below at each *epoch*. We then switch the model to evaluation mode, pass the test data to the model and evaluate how effectively the model classifies the unseen test data.

```{r nn_function}
predict <- function(embedding_dim, 
                    n_hidden, 
                    batch_size,
                    learn_rate,
                    num_epochs,
                    dropout_rate) {
  require(torch)
  require(tidyverse)
  set.seed(1)
  torch_manual_seed(1)
  #
  # model variables
  #
  max_len <- tensor_data_train$size(2)
  #
  # model setup
  #
  model <- nn_module(
    initialize = function(embed_dim, n_hidden, max_len) {
      self$embedding <- nn_embedding(
        length(vocab) + 2, 
        embed_dim, 
        padding_idx = length(vocab) + 1)
      self$hidden <- nn_linear(embed_dim * max_len, n_hidden)
      self$dropout <- nn_dropout(p = dropout_rate)  # 0.5
      self$output <- nn_linear(n_hidden, 1)
      self$sigmoid <- nn_sigmoid()
    },
    forward = function(x) {
      x <- self$embedding(x)  # embedding lookup
      x <- torch_flatten(x, start_dim = 2)  # flatten embeddings
      x <- self$hidden(x)  # Pass through the hidden layer
      x <- self$dropout(x)  # apply dropout
      x <- self$output(x)  # Pass through the output layer
      x <- self$sigmoid(x)  # Apply sigmoid to get probabilities
      return(x)
    }
  )
  #
  # instantiate
  #
  nn <- model(embedding_dim, n_hidden, max_len)
  rm(
    model, 
    embedding_dim,
    max_len, 
    n_hidden)
  #
  # dataloader
  #
  dataloader_train <- dataloader(
    dataset = enron_tensor_dataset_train,  # Your custom dataset
    batch_size = batch_size,         # The batch size you want to use
    shuffle = TRUE                   # Whether to shuffle the data
  )
  dataloader_test <- dataloader(
    dataset = enron_tensor_dataset_test,  # Your custom dataset
    batch_size = batch_size,         # The batch size you want to use
    shuffle = TRUE                   # Whether to shuffle the data
  )
  #
  # training
  #
  optimizer <- optim_adam(nn$parameters, lr = learn_rate)
  criterion <- nn_bce_loss()
  for (epoch in 1:num_epochs) {
    total_loss <- 0
    coro::loop(for (batch in dataloader_train) {
      batch_data <- 
        batch[[1]]$to(dtype = torch_int64()) + 
        torch_tensor(1, dtype = torch_int64())
      batch_labels <- batch[[2]]$to(dtype = torch_float32())
      optimizer$zero_grad()  # reset gradients
      output <- nn(batch_data) # forward pass
      loss <- criterion(output, batch_labels) # compute loss
      loss$backward() # backward pass and optimize
      optimizer$step()
      total_loss <- total_loss + loss$item()
    })
    cat("Epoch:", 
        epoch, 
        "Average Loss:", 
        total_loss / length(dataloader_train), 
        "\n")
  }
  nn$eval() # set model to evaluation mode
  with_no_grad({ # no need to compute gradients in eval mode
    train_scores <- 
      nn(tensor_data_train)$squeeze() # fwd pass, remove singleton dims
  })
  train_scores <- # create new variable
    as_array(train_scores) # convert scores tensor to r vector
  enron_train <- # update original dataset
    enron_train |> # pipe out original dataset
    mutate(score = train_scores) # add new column
  with_no_grad({ # no need to compute gradients in eval mode
    train_scores <- 
      nn(tensor_data_train)$squeeze() # fwd pass, remove singleton dims
  })
  train_scores <- # create new variable
    as_array(train_scores) # convert scores tensor to r vector
  enron_train <<- # update original dataset
    enron_train |> # pipe out original dataset
    mutate(score = train_scores) # add new column
  with_no_grad({
    test_scores <- 
      nn(tensor_data_test)$squeeze() # fwd pass, remove singleton dims
  })
  test_scores <- # create new variable
    as_array(test_scores) # convert scores tensor to r vector
  enron_test <<- # update original dataset
    enron_test |> # pipe out original dataset
    mutate(score = test_scores) # add new column
  rm(
    criterion, 
    optimizer, 
    num_epochs, 
    epoch,
    train_scores,
    test_scores)
  accuracy <- enron_test |>
    mutate(
      correct = (score > 0.5 & relevant == 1) | 
        (score <= 0.5 & relevant == 0)) |>
    summarize(correct_percentage = mean(correct)) |>
    as.numeric()
  return(accuracy)
}
predict(embedding_dim = 64, # best 64
        n_hidden = 64, # best 64
        batch_size = 500, # best 500
        learn_rate = 0.001, # best 0.001
        num_epochs = 6, # best 6
        dropout_rate = 0.5) # best 0.5
```


## Result Distribution

We provide a histogram showing distribution of test scores. We expect an effect model to have higher frequencies at extremes.
```{r result_distribution}
require(ggplot2) # for plotting
require(tidyverse) # for data manipulation
require(viridis)
ggplot(enron_test, aes(x = score, fill = ..count..)) +
  geom_histogram(binwidth = 0.1) +
  labs(title = "Histogram of Scores",
       x = "Score",
       y = "Frequency") +
  scale_fill_viridis_c(option = "viridis", end = 0.8)
```


## Result Sample

We also provide a sample of scores on the test dataset below. These are merely illustrative and not indicative of model performance.
```{r result_exerpt}
# exerpt
relevant_1 <- enron_test |>
  filter(relevant == 1) |>
  sample_n(5)
relevant_0 <- enron_test |>
  filter(relevant == 0) |>
  sample_n(5)
sampled_enron <- 
  bind_rows(relevant_1, relevant_0)
sampled_enron |>
  select(doc_id, relevant, score) |>
  as.data.frame()
rm(relevant_1,
   relevant_0,
   sampled_enron)
```

## Bayesian Hyperparameter Optimisation

```{r bayesian_optimisation}
require(rBayesianOptimization)
set.seed(1)
torch_manual_seed(1)
optimization_function <- 
  function(embedding_dim, 
           n_hidden, 
           batch_size, 
           learn_rate, 
           num_epochs,
           dropout_rate) {
  embedding_dim <- round(embedding_dim)
  n_hidden <- round(n_hidden)
  batch_size <- round(batch_size)
  num_epochs <- round(num_epochs)
  dropout_rate <- as.numeric(dropout_rate)
  accuracy <- predict(
    embedding_dim = embedding_dim,
    n_hidden = n_hidden,
    batch_size = batch_size,
    learn_rate = learn_rate,
    num_epochs = num_epochs,
    dropout_rate = dropout_rate
  )
  return(list(Score = accuracy, Pred = accuracy))
}
bounds <- list(
  embedding_dim = c(32L, 256L), # originally 23-128
  n_hidden = c(32L, 128L), # originally 32-128
  batch_size = c(50L, 2000L), # originally 100-1000
  learn_rate = c(0.0001, 0.01), # originally 0.0001-0.01
  num_epochs = c(1L, 20L), # originally 1-10
  dropout_rate = c(0.1, 0.5) # originally nailed at 0.5
)
bayesian_results <- 
  BayesianOptimization(
  FUN = optimization_function,
  bounds = bounds,
  init_points = 5,
  n_iter = 10,
  acq = "ei",
  verbose = FALSE
) |> invisible()
best_params <- bayesian_results$Best_Par
param_names <- names(best_params)
param_values <- as.numeric(best_params) |> round(2)
df_params <- data.frame(
  Variable = param_names,
  Value = param_values
)
best_value <- bayesian_results$Best_Value |> round(2)
df_value <- data.frame(
  Variable = "Best_Value",
  Value = best_value
)
final_df <- rbind(df_params, df_value)
rm(best_params, param_names, param_values, best_value, df_params, df_value)
print(final_df)
```
