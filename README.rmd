---
title: "\"oal\" - Occasional Active Learning"
subtitle: "An open source implementation of e-discovery active learning"
author: "Andre Abadi"
date: "`r Sys.Date()`"
output:
  github_document:
    df_print: kable
  pdf_document:
    fig_caption: true # render captions for figures
    extra_dependencies: ["float"] # floating images
    df_print: kable # print dataframes as kables automatically
---

```{r setup_output, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, # knit code (default = TRUE)
  message = FALSE, # knit messages (default = TRUE)
  warning = FALSE, # knit warnings (default = TRUE)
  cache = TRUE, # cache knitted chunks (default = FALSE)
  fig.align = "centre", # align figures to centre
  fig.pos = "H", # hard-here-mode for plots
  out.extra = ""
)
```

```{r setup_code}
set.seed(1)
library(tidyverse)
library(viridis)
library(tidymodels)
tidymodels_prefer()
library(textrecipes)
library(discrim)
library(naivebayes)
```

## Introduction

The aim of this project is to develop a predictive model for binary classification of legal documents. Many e-discovery platforms implement such predictive models but are proprietary. This project aims to provide an open source equivalent, not for production (necessarily) but for educational purposes. We hope to meet or exceed the following metrics:

| .metric | estimate |
| --- | --- |
| accuracy | 0.74 |
| roc_auc | 0.77 |

### References

Our primary text reference for this project is *Supervised Machine Learning for Text Analysis in R* (**SMLTAR**) by Emil Whitfield and Julia Silge. This text is available as an interactive resource at [smltar.com](https://smltar.com/).

### Style and Conventions

- Code is written using `snake_case` and attempts to follow the guidance of the online resource [*R for Data Science (2e)*](https://r4ds.hadley.nz/) by Wickham, Ã‡etinkaya-Rundel, and Grolemund.
- We also adopt the [*Tidyverse Style Guide*](https://style.tidyverse.org/) with a preference toward piping operations where possible for code clarity.
- We use the native pipe where possible and fall back to the [*magrittr*](https://magrittr.tidyverse.org/) pipe where necessary.
- We also use the [*viridis*](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html), colourising charts to improve accessibility of visualisations for readers with colour vision deficiency.
- We use [Tidymodels](https://www.tidymodels.org/) for our analysis, as it promotes good modelling practices by streamlining workflows, minimizing data leakage risks, and ensuring reproducibility.

## Import

```{r import}
require(tidyverse)
enron_raw <-
  read_csv("data/enron_5k_inboard.csv",
           show_col_types = FALSE) |>
  select(-md5) |>
  mutate(relevant = as_factor(relevant),
         train_test = as_factor(train_test))
enron_raw |> glimpse()
```
## Prepare

### Tidy

```{r tidy}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
enron_tidy <-
  enron_raw |>
  select(-subject,-date,-from,-to) |>
  filter(!is.na(train_test))
enron_tidy |> glimpse()
```

### Splits and Folds

```{r splits_and_folds}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
train_indices <- 
  which(enron_tidy$train_test == "train")
test_indices <- 
  which(enron_tidy$train_test == "test")
enron_split <- 
  make_splits(list(analysis = train_indices, 
                   assessment = test_indices), 
              enron_tidy)
rm(train_indices,test_indices)
enron_train <-
  training(enron_split)
enron_test <-
  testing(enron_split)
enron_folds <-
  vfold_cv(enron_train)
```

### Preprocessing Recipes

Our first recipe is a basic recipe for ingestion to the null model, and is unfiltered tokenization truncated at the top 1000 with TF-IDF subsequently applied. We do not propose to use it for any true modelling.

```{r r00}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
recipe_null <-
  recipe(relevant ~ content, data = enron_train) |>
  step_tokenize(content) |>
  step_tokenfilter(content, max_tokens = 1e3) |>
  step_tfidf(content)
recipe_null |>
  prep() |>
  bake(new_data = NULL) |>
  select(contains("content")) |>
  slice_head(n = 5) |>
  select_if(~any(. != 0)) |>
  select(1:5) |>
  as.data.frame()
```
The second recipe is similar to the first, with unfiltered tokenization, but this time with the top *n* tokens set for tuning between 500 and 5000, and TF-IDF applied subsequently as with the previous recipe.

```{r r01}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
recipe_tfidf <-
  recipe(relevant ~ content, data = enron_train) |>
  step_tokenize(content) |>
  step_tokenfilter(content, max_tokens = tune()) |>
  step_tfidf(content)
```


## Modelling

### Model 00 - Null Model

[SMLTAR 7.2](https://smltar.com/mlclassification#classnull)

>"a 'null model' or baseline model, [is] a simple, non-informative model that always predicts the largest class for classification. Such a model is perhaps the simplest heuristic or rule-based alternative that we can consider as we assess our modelling efforts."

```{r m00}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
null_model <-
  null_model() |>
  set_engine("parsnip") |>
  set_mode("classification")
null_workflow <-
  workflow() |>
  add_recipe(recipe_null) |>
  add_model(null_model)
null_fit <-
  null_workflow |>
  fit_resamples(
    resamples = enron_folds,
    control = control_resamples(save_pred = TRUE)
  )
null_metrics <-
  null_fit |> 
  collect_metrics()
null_metrics |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
rm(
  null_model,
  null_workflow,
  null_fit,
  null_metrics
)
```

The null model above shows a ROC_AUC indicating that without any predictive modelling, the null model guesses the correct classification that proportion of the time. This is consistent with the 50:50 breakdown of labelled training data. It is akin to flipping a coin, and provides our baseline that we hope to improve upon.

### Model 01 - Naive Bayes

[SMLTAR 7.1.1](https://smltar.com/mlclassification#classfirstmodel)

A Naive Bayes classifier applies Bayes' Theorem to predict class membership by calculating conditional probabilities, making the "naive" assumption that features are conditionally independent of each other. Despite this simplifying assumption, the classifier is remarkably effective, particularly for text classification tasks. It works by learning the probability of each feature occurring within each class during training, then combining these probabilities with prior class probabilities to predict the most likely class for new instances.

```{r m01}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
require(textrecipes)
require(discrim)
require(naivebayes)
bayes_model <-
  naive_Bayes() |>
  set_mode("classification") |>
  set_engine("naivebayes")
bayes_workflow <-
  workflow() |>
  add_recipe(recipe_tfidf) |>
  add_model(bayes_model)
bayes_grid <-
  grid_regular(
    max_tokens(range = c(100,5000)),
    levels = 5
  )
bayes_resamples <-
  bayes_workflow |>
  tune_grid(
    resamples = enron_folds,
    grid = bayes_grid,
    control = control_resamples(save_pred = TRUE)
  )
bayes_best <-
  bayes_resamples |>
  select_by_one_std_err(metric = "roc_auc", -max_tokens)
bayes_final_workflow <-
  finalize_workflow(
    bayes_workflow,
    bayes_best 
  )
bayes_final_fit <-
  bayes_final_workflow |>
  fit_resamples(
    resamples = enron_folds,
    control = control_resamples(save_pred = TRUE)
  )
bayes_metrics <-
  bayes_final_fit |>
  collect_metrics()
bayes_roc <- 
  bayes_final_fit |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(relevant, .pred_1) |>
  mutate(model = "Naive Bayes")
bayes_metrics |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
bayes_final_fit |>
  collect_predictions() |> 
  conf_mat(truth = relevant, 
          estimate = .pred_class) |>
  autoplot(type = "heatmap") +
  scale_fill_viridis_c(begin = 0.3) +
  scale_x_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  scale_y_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  labs(
    title = "Confusion Matrix for Model 00 - Naive Bayes",
    x = "Truth",
    y = "Prediction"
  )
rm(
  bayes_model,
  bayes_workflow,
  bayes_grid,
  bayes_resamples,
  bayes_best,
  bayes_final_workflow,
  bayes_metrics
)
```

Analysing the above results we see that there is reasonable but unbalanced performance. The model was effective at classifying the irrelevant documents but did not perform well at classifying the relevant documents. Given the balanced training data, this suggests the model did not perform well at this task.

### Model 02 - LASSO

[SMLTAR 7.3](https://smltar.com/mlclassification#comparetolasso)

>"Regularized linear models are a class of statistical model that can be used in regression and classification tasks. Linear models are not considered cutting edge in NLP research, but are a workhorse in real-world practice. Here we will use a lasso regularized model (Tibshirani 1996), where the regularization method also performs variable selection. In text analysis, we typically have many tokens, which are the features in our machine learning problem."

```{r m02}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
set.seed(1)
lasso_model <-
  logistic_reg(penalty = tune(), mixture = 1) |>
  set_mode("classification") |>
  set_engine("glmnet")
lasso_workflow <- 
  workflow() |>
  add_recipe(recipe_tfidf) |>
  add_model(lasso_model)
lasso_grid <- 
  grid_regular(
    max_tokens(range = c(500, 2000)),
    penalty(range = c(-4, 0)),
    levels = c(3, 3)
  )
lasso_resamples <-
  lasso_workflow |>
  tune_grid(
    resamples = enron_folds,
    grid = lasso_grid,
    control = control_resamples(save_pred = TRUE)
  )
lasso_best <-
  lasso_resamples |>
  select_by_one_std_err(metric = "roc_auc", -penalty)
lasso_final_workflow <-
  finalize_workflow(
    lasso_workflow,
    lasso_best
  )
lasso_final_fit <-
  lasso_final_workflow |>
  fit_resamples(
    resamples = enron_folds,
    control = control_resamples(save_pred = TRUE)
  )
lasso_final_fit |>
  collect_metrics() |>
  select(.metric,mean,std_err) |>
  filter(.metric != "brier_class") |>
  as.data.frame()
lasso_final_fit |>
  collect_predictions() |> 
  conf_mat(truth = relevant, 
          estimate = .pred_class) |>
  autoplot(type = "heatmap") +
  scale_fill_viridis_c(begin = 0.3) +
  scale_x_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  scale_y_discrete(labels = c("0" = "Irrelevant", "1" = "Relevant")) +
  labs(
    title = "Confusion Matrix for Model 01 - LASSO Regularised Model",
    x = "Truth",
    y = "Prediction"
  )
rm(
  lasso_model,
  lasso_workflow,
  lasso_grid,
  lasso_resamples,
  lasso_best,
  lasso_final_workflow
)
```

In the above results we see that the LASSO model produces more balanced classifications between Relevant and Irrelevant. The regularisation penalty was tuned to achieve the above result. We see a small increase in the accuracy metric, and a significant improvement in the ROC_AUC metric. This, combined with the subjective assessment via the confusion matrix suggests an improved model overall.

## Comparison

We show the ROC curve of our models below.

```{r roc_compare}
require(tidyverse)
require(tidymodels)
tidymodels_prefer()
bayes_roc <- bayes_final_fit |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(truth = relevant, .pred_0) |>
  mutate(model = "Model 01 - Naive Bayes")
lasso_roc <- lasso_final_fit |>
  collect_predictions() |>
  group_by(id) |>
  roc_curve(truth = relevant, .pred_0) |>
  mutate(model = "Model 02 - LASSO")
bind_rows(bayes_roc, lasso_roc) |>
  group_by(model, 
          specificity = round(specificity, 2)) |>
  summarize(
    sensitivity_mean = mean(sensitivity),
    sensitivity_lower = quantile(sensitivity, 0.025),
    sensitivity_upper = quantile(sensitivity, 0.975),
    .groups = "drop"
  ) |>
  ggplot(aes(x = 1 - specificity, 
             y = sensitivity_mean, 
             color = model,
             fill = model)) +
#  geom_ribbon(aes(ymin = sensitivity_lower,
#                  ymax = sensitivity_upper),
#              alpha = 0.2,
#              color = NA) +
  geom_line(linewidth = 1) +
  geom_abline(lty = 3) +
  coord_equal() +
  scale_color_viridis_d(end = 0.8) +
  scale_fill_viridis_d(end = 0.8) +
  labs(
    title = "ROC Curve Comparison",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Model",
    fill = "Model"
  )
rm(
  bayes_roc,
  lasso_roc
)
```
